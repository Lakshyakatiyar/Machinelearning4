{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a20d610-9899-45be-8460-8d7f095a9a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "X_min = 1\n",
    "X_max = 20\n",
    "X_scaled = [(x - X_min) / (X_max - X_min) for x in [1, 5, 10, 15, 20]]\n",
    "X_new = [2 * x - 1 for x in X_scaled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57c58a46-b696-417d-8237-a04c035724d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0, -0.5789473684210527, -0.052631578947368474, 0.4736842105263157, 1.0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c195f9d-aa6d-40eb-8c09-75d5d7d8f852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        PC1       PC2  gender\n",
      "0  0.018144 -0.553007    Male\n",
      "1 -0.773906  0.974704  Female\n",
      "2  2.926254  0.254197    Male\n",
      "3 -2.833733 -0.134058  Female\n",
      "4  0.663240 -0.541836    Male\n"
     ]
    }
   ],
   "source": [
    "#8=Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform a dataset into a new coordinate system (principal components) that captures the most important information while minimizing loss of variance. When performing PCA, one of the critical decisions is determining how many principal components to retain.\n",
    "\n",
    "#Here's how you could approach feature extraction using PCA for the given dataset with features [height, weight, age, gender, blood pressure]:\n",
    "\n",
    "#Data Preprocessing:\n",
    "\n",
    "#Standardize the features (subtract mean and divide by standard deviation) to ensure they're on the same scale, as PCA is sensitive to feature scaling.\n",
    "#Perform PCA:\n",
    "\n",
    "#Apply PCA to the standardized dataset. This will yield a set of principal components, each representing a linear combination of the original features.\n",
    "#Explained Variance:\n",
    "\n",
    "#Calculate the explained variance ratio for each principal component. The explained variance ratio indicates the proportion of total variance explained by each component.\n",
    "#Choose Number of Components:\n",
    "\n",
    "#Decide on the number of principal components to retain. One common approach is to choose the smallest number of components that explain a significant portion of the total variance.\n",
    "#The number of principal components to retain depends on the desired trade-off between dimensionality reduction and the amount of variance retained. Commonly used methods for determining the number of components include:\n",
    "\n",
    "#Cumulative Explained Variance:\n",
    "\n",
    "#Plot the cumulative explained variance ratio and choose the number of components where the curve starts to level off. Retaining components that collectively explain a large portion of the variance is often a good strategy.\n",
    "#Elbow Method:\n",
    "\n",
    "#Plot the explained variance ratio against the number of components. Choose the number of components where the explained variance stops increasing significantly, resembling an \"elbow\" point.\n",
    "#Domain Knowledge:\n",
    "\n",
    "#If you have domain expertise or prior knowledge about the dataset, it can guide you in choosing the number of components. For instance, if certain features are inherently more important, you might retain components that capture their influence.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset (replace with your actual data)\n",
    "data = pd.DataFrame({\n",
    "    'height': [170, 165, 180, 155, 175],\n",
    "    'weight': [65, 55, 75, 50, 70],\n",
    "    'age': [30, 25, 40, 22, 28],\n",
    "    'gender': ['Male', 'Female', 'Male', 'Female', 'Male'],\n",
    "    'blood_pressure': [120, 130, 140, 110, 125]\n",
    "})\n",
    "\n",
    "# Extract numerical features\n",
    "numerical_features = ['height', 'weight', 'age', 'blood_pressure']\n",
    "numerical_data = data[numerical_features]\n",
    "\n",
    "# Standardize the numerical features\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(numerical_data)\n",
    "\n",
    "# Perform PCA\n",
    "num_components = 2  # Choose the number of components to retain\n",
    "pca = PCA(n_components=num_components)\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Create a new DataFrame with the principal components\n",
    "pc_df = pd.DataFrame(data=principal_components, columns=[f'PC{i}' for i in range(1, num_components+1)])\n",
    "\n",
    "# Concatenate the principal components DataFrame with other data (e.g., gender)\n",
    "final_data = pd.concat([pc_df, data[['gender']]], axis=1)\n",
    "\n",
    "print(final_data)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b264656f-e5eb-49a9-9507-de02848b9030",
   "metadata": {},
   "source": [
    "6=Using Principal Component Analysis (PCA) to reduce the dimensionality of a dataset for predicting stock prices involves transforming the original features into a lower-dimensional space while retaining as much variance as possible. This can help improve model performance, reduce computational complexity, and mitigate the curse of dimensionality. Here's how you could use PCA for dimensionality reduction in your stock price prediction project:\n",
    "\n",
    "Step 1: Data Preprocessing:\n",
    "\n",
    "Collect and preprocess the dataset, which includes various features related to company financial data and market trends.\n",
    "Step 2: Standardization:\n",
    "2. Standardize the features to ensure that they are on the same scale. This step is crucial because PCA is sensitive to the scale of features.\n",
    "\n",
    "Step 3: Apply PCA:\n",
    "3. Apply PCA to the standardized dataset. The goal is to find the principal components that capture the most important information in the data.\n",
    "\n",
    "Step 4: Explained Variance:\n",
    "4. Calculate the explained variance ratio for each principal component. The explained variance ratio tells you how much of the total variance in the original data is captured by each principal component.\n",
    "\n",
    "Step 5: Determine Number of Components:\n",
    "5. Decide how many principal components to retain. You can use techniques like cumulative explained variance or the elbow method to help determine the appropriate number.\n",
    "\n",
    "Step 6: Transform Data:\n",
    "6. Transform the original data into the lower-dimensional space defined by the retained principal components.\n",
    "\n",
    "Step 7: Model Building:\n",
    "7. Use the transformed data as input for your stock price prediction model. The reduced feature set can potentially lead to improved model performance and faster training times.\n",
    "\n",
    "Step 8: Inverse Transform (Optional):\n",
    "8. If necessary, you can use the inverse transform to map the predictions back into the original feature space for better interpretability.\n",
    "\n",
    "Benefits of PCA in Stock Price Prediction:\n",
    "\n",
    "Dimensionality Reduction: PCA can help reduce the number of features while retaining much of the relevant information.\n",
    "Noise Reduction: PCA can remove noise and irrelevant information from the data.\n",
    "Collinearity Handling: PCA can handle collinearity among features, which can be common in financial and market data.\n",
    "Model Performance: The reduced dimensionality can lead to faster model training and potentially better generalization to unseen data.\n",
    "Interpretability: Although the principal components themselves might not be directly interpretable, the transformed data can still provide insights.\n",
    "Considerations:\n",
    "\n",
    "Be cautious with dimensionality reduction, as you might lose some interpretability of the features.\n",
    "PCA assumes that the most important information is captured by the first few principal components. Ensure that this assumption holds in your specific context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8453385-d07f-4b14-aac3-db01923ac875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   price    rating  delivery_time\n",
      "0   0.00  0.777778            0.2\n",
      "1   0.50  0.000000            0.8\n",
      "2   0.25  0.444444            0.6\n",
      "3   0.75  1.000000            0.0\n",
      "4   1.00  0.222222            1.0\n"
     ]
    }
   ],
   "source": [
    "#5=\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataset (replace with your actual data)\n",
    "data = pd.DataFrame({\n",
    "    'price': [10, 20, 15, 25, 30],\n",
    "    'rating': [4.5, 3.8, 4.2, 4.7, 4.0],\n",
    "    'delivery_time': [30, 45, 40, 25, 50]\n",
    "})\n",
    "\n",
    "# Extract numerical features\n",
    "numerical_features = ['price', 'rating', 'delivery_time']\n",
    "numerical_data = data[numerical_features]\n",
    "\n",
    "# Apply Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(numerical_data)\n",
    "\n",
    "# Create a new DataFrame with the scaled features\n",
    "scaled_df = pd.DataFrame(data=scaled_data, columns=numerical_features)\n",
    "\n",
    "print(scaled_df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5af7efb-a191-4598-9b76-afb671b8b009",
   "metadata": {},
   "source": [
    "4=Principal Component Analysis (PCA) is a dimensionality reduction technique that can also be used for feature extraction. The relationship between PCA and feature extraction lies in the fact that PCA aims to transform the original features into a new set of features (principal components) that capture the most important information in the data. These principal components can then serve as a reduced set of features that retain as much variance as possible.\n",
    "\n",
    "PCA for Feature Extraction:\n",
    "PCA can be used as a feature extraction technique by transforming the original high-dimensional feature space into a lower-dimensional space while preserving as much information as possible. This can be especially useful when dealing with datasets with many correlated features or when you want to reduce the complexity of your data while maintaining the essential characteristics.\n",
    "\n",
    "Here's an example of how PCA can be used for feature extraction:\n",
    "\n",
    "Example: Image Compression\n",
    "Consider a dataset of images, where each pixel value is treated as a feature. These images have high-dimensional feature spaces, making them computationally intensive to process. By using PCA for feature extraction, you can transform the pixel values into principal components that capture the main patterns and variations in the images.\n",
    "\n",
    "Data: A set of grayscale images represented as pixel values.\n",
    "\n",
    "Feature Space: Each pixel is a feature, resulting in a high-dimensional feature space.\n",
    "\n",
    "Apply PCA for Feature Extraction:\n",
    "\n",
    "Standardize the pixel values across all images.\n",
    "Apply PCA to the standardized data.\n",
    "Calculate the principal components.\n",
    "Reduced Feature Space: The principal components represent a lower-dimensional feature space that captures the most important variations in the images.\n",
    "\n",
    "Dimensionality Reduction: Choose the number of principal components to retain. These principal components are effectively a compressed representation of the original images.\n",
    "\n",
    "Use for Analysis: The principal components can be used for various tasks like image reconstruction, clustering, or classification.\n",
    "\n",
    "In this example, PCA is used to extract the essential information from the images and reduce their dimensionality. The principal components serve as the new set of features that capture the main patterns in the images."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d580e1f-7918-44fe-a010-c1f58fcd8d9f",
   "metadata": {},
   "source": [
    "3=Principal Component Analysis (PCA) is a dimensionality reduction technique widely used in various fields to transform high-dimensional data into a lower-dimensional space while retaining the most important information. PCA achieves this by finding a set of orthogonal axes (principal components) along which the data varies the most. These principal components capture the directions of maximum variance in the data, allowing you to represent the data in a reduced number of dimensions.\n",
    "\n",
    "How PCA Works for Dimensionality Reduction:\n",
    "\n",
    "Centering Data: PCA starts by centering the data, ensuring that each feature has a mean of zero. This step removes any biases in the data.\n",
    "\n",
    "Covariance Matrix: PCA calculates the covariance matrix of the centered data. The covariance matrix shows how different features vary in relation to one another.\n",
    "\n",
    "Eigenvalue Decomposition: PCA performs eigenvalue decomposition on the covariance matrix. This yields eigenvalues and corresponding eigenvectors. Eigenvectors represent the directions of maximum variance (principal components).\n",
    "\n",
    "Selecting Principal Components: PCA sorts the eigenvalues in decreasing order. The eigenvectors corresponding to the largest eigenvalues are the principal components that capture the most variance in the data.\n",
    "\n",
    "Reduced Dimensionality: You can choose to retain a subset of the principal components, effectively reducing the dimensionality of the data. These principal components form a new coordinate system.\n",
    "\n",
    "Projection: Data is projected onto the new coordinate system formed by the selected principal components. This projection results in data points with reduced dimensions but retaining the most important information.\n",
    "\n",
    "Example: Dimensionality Reduction for 2D Data:\n",
    "Consider a dataset of 2D points representing a cloud of data in a scatter plot. The original data is spread along both axes. Using PCA, you can find the directions of maximum variance, which are the principal components. Let's illustrate this with an example:\n",
    "\n",
    "Original Data: Points in a 2D scatter plot.\n",
    "\n",
    "Covariance Matrix: Calculate the covariance matrix based on the data.\n",
    "\n",
    "Eigenvalue Decomposition: Find the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "Principal Components: The eigenvectors represent the directions of maximum variance. These are the principal components.\n",
    "\n",
    "Reduced Dimensionality: Choose the number of principal components to retain. In this case, let's retain only one principal component.\n",
    "\n",
    "Projection: Project the data onto the selected principal component. This reduces the data from 2D to 1D.\n",
    "\n",
    "As a result, you have transformed the original data into a lower-dimensional space while retaining the essential information. This reduction can aid in visualizing, analyzing, or modeling the data more efficiently.3="
   ]
  },
  {
   "cell_type": "raw",
   "id": "eba00b33-9b4a-4b62-b8f9-4e92d609f677",
   "metadata": {},
   "source": [
    "2= \"Unit Vector\" technique in feature scaling, also known as \"Normalization,\" is a method that scales the features of a dataset to ensure that each data point lies on the surface of a unit hypersphere. This means that after scaling, each data point is transformed to have a length (magnitude) of 1, preserving the direction of the original vector while normalizing its magnitude. This technique is particularly useful when dealing with features measured in different units or with varying ranges.\n",
    "\n",
    "The Unit Vector technique differs from Min-Max scaling, which scales features to a specific range (usually between 0 and 1). While Min-Max scaling preserves the original direction of the data points, it doesn't necessarily ensure that all points have the same length, and the resulting vectors might not lie on a unit hypersphere.\n",
    "\n",
    "How Unit Vector (Normalization) Scaling Works:\n",
    "The unit vector of a vector v is calculated as:\n",
    "\n",
    "scss\n",
    "Copy code\n",
    "unit_vector(v) = v / ||v||\n",
    "where ||v|| represents the Euclidean norm (magnitude) of the vector v.\n",
    "\n",
    "Comparison with Min-Max Scaling:\n",
    "\n",
    "Min-Max Scaling: Scales features to a specific range, such as [0, 1].\n",
    "Unit Vector Scaling: Scales features such that each data point has a length of 1, while preserving the direction.\n",
    "Example: Unit Vector Scaling vs. Min-Max Scaling:\n",
    "\n",
    "Suppose you have a dataset with two features: \"height\" and \"weight.\" Here are the original values:\n",
    "\n",
    "less\n",
    "Copy code\n",
    "Height: [160, 170, 180, 155]\n",
    "Weight: [50, 65, 70, 45]\n",
    "Unit Vector Scaling:\n",
    "To scale the data using the unit vector technique:\n",
    "\n",
    "Calculate the magnitude of each data point: ||(height, weight)|| = √(height^2 + weight^2)\n",
    "Divide each data point by its magnitude to get the unit vector.\n",
    "Min-Max Scaling:\n",
    "To scale the data using Min-Max scaling:\n",
    "\n",
    "Calculate the minimum and maximum values for each feature.\n",
    "Apply the Min-Max scaling formula to each feature.\n",
    "Comparison:\n",
    "\n",
    "Unit Vector Scaling results in data points that have a length of 1 while preserving direction.\n",
    "Min-Max Scaling scales the data to a specified range.\n",
    "For the sake of this illustration, let's assume the calculated values are as follows:\n",
    "\n",
    "Unit Vector Scaled Data:\n",
    "\n",
    "Unit Vector Scaled Height: [0.98058068, 0.93933638, 0.9486833, 0.99227788]\n",
    "Unit Vector Scaled Weight: [0.8660254, 0.8660254, 0.70710678, 0.70710678]\n",
    "Min-Max Scaled Data:\n",
    "Min-Max Scaled Height: [0.5, 0.75, 1.0, 0.0]\n",
    "Min-Max Scaled Weight: [0.5, 1.0, 1.0, 0.0]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930d285a-ec7f-41bd-9c98-29858c105b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "1="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
